{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig. 3\n",
    "\n",
    "\n",
    "\n",
    "**Method**\n",
    "\n",
    "- PC1 and 2 are the 1st and 2nd principal components of the natural image dataset.\n",
    "\n",
    "\n",
    "Execution time: 10 secs\n",
    "\n",
    "TO DO:\n",
    "- prior is not zero mean but close -> keep like this?\n",
    "\n",
    "DONE:\n",
    "- same prior sigma\n",
    "- same grid_x and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "setup fisher_info_limits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import exp, ceil, sqrt, log, pi, zeros, eye, linspace, meshgrid, trapezoid\n",
    "from numpy.linalg import solve, slogdet, inv\n",
    "from scipy.special import gammaln\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as  plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import wilcoxon, ks_2samp, multivariate_normal\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.signal import convolve2d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from numba import njit, prange\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# set project path\n",
    "main_dir = '/home/steeve/steeve/idv/code/fisher-info-limits'\n",
    "os.chdir(main_dir)\n",
    "\n",
    "# pipeline parameters\n",
    "LOAD_METRICS = True\n",
    "GRID_POS = 220\n",
    "\n",
    "# path\n",
    "CELL_DATA_PATH = 'data/contrast_cells/carlo_data_cellno2.mat' # raw cell receptive field data\n",
    "METRICS_DATA_PATH = 'data/computed_contrast_cells/BDEvSSI_no2.npz' # precomputed metrics\n",
    "\n",
    "# setup figure paraleters\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"lines.linewidth\"] = 0.5\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.5\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"xtick.major.width\"] = 0.5 \n",
    "plt.rcParams[\"xtick.minor.width\"] = 0.5 \n",
    "plt.rcParams[\"ytick.major.width\"] = 0.5 \n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.5\n",
    "plt.rcParams[\"xtick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"xtick.minor.size\"] = 2 * 1.1\n",
    "plt.rcParams[\"ytick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"ytick.minor.size\"] = 2 * 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils -----------------------------------------------------------\n",
    "\n",
    "def vec(data: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"flatten tensor in column-major order into\n",
    "    a 2 dimensional tensor (N x 1)\n",
    "\n",
    "    Args:\n",
    "        data (torch.tensor): N x M array \n",
    "\n",
    "    Returns:\n",
    "        (torch.tensor): ((N*M), 1) vector tensor\n",
    "    \"\"\"\n",
    "    return data.T.flatten()[:,None]    \n",
    "\n",
    "\n",
    "def sum2(data):\n",
    "    \"\"\"Sum over columns and keep \n",
    "    data dimensionality\n",
    "    \"\"\"\n",
    "    return (data).sum(1, keepdims=True)\n",
    "\n",
    "\n",
    "def create_gaussian_mask(grid_x, grid_y, mu, sigma, n_std=3):\n",
    "    \"\"\"\n",
    "    Create a mask for points outside n standard deviations of a 2D Gaussian.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_x, grid_y : array_like\n",
    "        Meshgrid coordinates\n",
    "    mu : array_like\n",
    "        Mean [mu_x, mu_y]\n",
    "    sigma : array_like\n",
    "        Covariance matrix (2x2)\n",
    "    n_std : float\n",
    "        Number of standard deviations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mask : array_like\n",
    "        Boolean mask (True = inside, False = outside)\n",
    "    \"\"\"\n",
    "    # Flatten grid for computation\n",
    "    points = np.stack([grid_x.ravel(), grid_y.ravel()], axis=1)\n",
    "    \n",
    "    # Calculate Mahalanobis distance\n",
    "    diff = points - mu\n",
    "    inv_sigma = np.linalg.inv(sigma)\n",
    "    mahal_dist_sq = np.sum(diff @ inv_sigma * diff, axis=1)\n",
    "    \n",
    "    # Points inside n_std satisfy: mahal_dist^2 <= n_std^2\n",
    "    mask = mahal_dist_sq <= n_std**2\n",
    "    return mask.reshape(grid_x.shape)\n",
    "\n",
    "\n",
    "def logsumexp(a: np.array, axis=0):\n",
    "    \"\"\"\n",
    "    Computes log(sum(exp(a), axis)) while avoiding numerical underflow.\n",
    "    Equivalent to MATLAB's logsumexp by Tom Minka.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        Input array.\n",
    "    axis : int, optional\n",
    "        Axis along which to sum. Default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    s : np.ndarray\n",
    "        Log-sum-exp of the input along the specified axis.\n",
    "    \"\"\"\n",
    "    # find the maximum along the specified axis (for numerical stability)\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "\n",
    "    # subtract and exponentiate\n",
    "    a_stable = a - a_max\n",
    "    s = a_max + log(np.sum(np.exp(a_stable), axis=axis, keepdims=True))\n",
    "\n",
    "    # remove the extra dimension\n",
    "    s = np.squeeze(s, axis=axis)\n",
    "    a_max = np.squeeze(a_max, axis=axis)\n",
    "\n",
    "    # handle non-finite max values (like -inf)\n",
    "    return np.where(np.isfinite(a_max), s, a_max)\n",
    "\n",
    "\n",
    "# 2D Poisson  -------------------------------------------\n",
    "\n",
    "\n",
    "def SUM_LOG_LIST(position):\n",
    "    '''Given an integer n it recursively calculates log(n!)'''\n",
    "    if position == 0:\n",
    "        return np.array([0])\n",
    "    if position == 1:\n",
    "        return np.append(SUM_LOG_LIST(0), 0)\n",
    "    new_list = SUM_LOG_LIST(position-1)\n",
    "    return np.append(new_list, new_list[-1]+np.around(np.log(float(position)), 8))\n",
    "\n",
    "\n",
    "def POISSON_2DCELL(tc_grid, max_firing=20):\n",
    "    log_list = np.tile(SUM_LOG_LIST(max_firing)[:,None,None], tc_grid.shape)\n",
    "    log_tc = np.around(np.log(tc_grid), 8)#, where=(mask==1), out = np.ones_like(tc_grid)*-100)\n",
    "    log_likelihood = (np.array([(i*log_tc-tc_grid) for i in range(max_firing+1)])-log_list)\n",
    "    likelihood = np.exp(log_likelihood)\n",
    "    likelihood = likelihood/np.sum(likelihood, axis=0)\n",
    "    return likelihood    \n",
    "\n",
    "\n",
    "# Neural network model of tuning curve ---------------------------\n",
    "\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def nnet_fit(pcs, fit, test_size=0.2, seed=42, n_epochs=200, linspace=np.linspace(-3,3,301)):\n",
    "    \"\"\"Fit tuning curves with a neural network\n",
    "\n",
    "    Args:\n",
    "        pcs (_type_): principal components of the cell's receptive field\n",
    "        fit (_type_): tuning curve\n",
    "        test_size (float, optional): _description_. Defaults to 0.2.\n",
    "        seed (int, optional): _description_. Defaults to 42.\n",
    "        n_epochs (int, optional): _description_. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # setup reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # get the predictors and predicted data\n",
    "    X = np.copy(pcs).T          # x and y\n",
    "    y = np.copy(np.array(fit[0]))   # response to fit\n",
    "    \n",
    "    # normalize principal components\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # create train/test split\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # convert to tensors\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32, requires_grad=True)\n",
    "    X_test  = torch.tensor(X_test_np, dtype=torch.float32, requires_grad=True)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "    y_test  = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # initialize model, loss, and optimizer\n",
    "    model = SimpleRegressor()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 20 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            val_loss = criterion(model(X_test), y_test).item()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # inference: evaluate on a grid for visualization\n",
    "    model.eval()\n",
    "\n",
    "    # calculate grid (will be used throughout)\n",
    "    grid_x, grid_y = np.meshgrid(linspace, linspace)\n",
    "    grid_input = np.column_stack((grid_x.ravel(), grid_y.ravel()))\n",
    "    grid_input_scaled = scaler.transform(grid_input)\n",
    "\n",
    "    # predict response\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(grid_input_scaled, dtype=torch.float32)).numpy()\n",
    "        \n",
    "    grad_X_tensor = torch.tensor(grid_input_scaled, dtype=torch.float32, requires_grad=True)\n",
    "    output = model(grad_X_tensor)\n",
    "    output.backward(torch.ones_like(output))\n",
    "    grads = grad_X_tensor.grad.detach().numpy()\n",
    "    return preds, model, grads, grid_x, grid_y, grid_input_scaled, grid_input\n",
    "\n",
    "\n",
    "# Neural encoder model ------------------------------------\n",
    "\n",
    "def tuning_curve_nnet(grid_input: np.array, model):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        grid_input (np.array): first 2 principal components of shape (2, N)\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # predict average firing rates\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(grid_input, dtype=torch.float32)).numpy()[:,0]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def tuning_curve_mog(x, model=None, mu=np.array([0.3, -1.2]), \n",
    "      Sigma_f=np.array([[1, 0.2], [0.2, 0.5]]), \n",
    "      Sigma_f2=np.array([[1, -0.4], [-0.4, 0.5]]), \n",
    "      amp=20):\n",
    "    \"\"\"Compute 2D tuning curve for a single neuron\n",
    "    as a mixture of two Gaussians.\n",
    "\n",
    "    The tuning curve models neural responses as a function of two stimulus dimensions\n",
    "    (e.g., natural image principal components). It combines two Gaussian bumps with\n",
    "    different centers and covariance structures, plus a baseline firing rate.    \n",
    "\n",
    "    Args:\n",
    "        x (np.array): Stimulus coordinates of shape (N, 2), where N is the number of points\n",
    "        ...and the two columns represent PC1 and PC2 (or any 2D feature space).\n",
    "        mu (np.array, optional): Center of the second Gaussian component, shape (2,). \n",
    "        ...Defaults to np.array([0.3, -1.2]).\n",
    "        Sigma_f (np.array, optional): Covariance (2x2) for the first Gaussian \n",
    "        ...component centered at origin. Controls the shape and orientation of \n",
    "        ...the first tuning bump. \n",
    "        ...Defaults to np.array([[1, 0.2], [0.2, 0.5]]).\n",
    "        Sigma_f2 (np.array, optional): Covariance matrix (2x2) for the second \n",
    "        ...Gaussian component centered at mu. Controls the shape and orientation \n",
    "        ...of the second tuning bump. \n",
    "        ...Defaults to np.array([[1, -0.4], [-0.4, 0.5]]). \n",
    "        amp (int, optional): Maximum amplitude (peak firing rate) \n",
    "        ...of the tuning curve in spikes/second. \n",
    "        ...Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Mean firing rates at each stimulus location (tuning curve), shape (N,).\n",
    "        Values represent spikes/second, with a baseline of 2 spikes/s.\n",
    "    \"\"\"\n",
    "    return amp * (exp(-0.5 * sum(x.T * solve(Sigma_f,x.T))) + exp(-0.5 * sum((x-mu).T * solve(Sigma_f2, (x-mu).T)))) / 2 + 2\n",
    "\n",
    "\n",
    "def get_tuning_curve_grad(X, tuning_curve_fun, model=None, eta=1e-4):\n",
    "    \"\"\"compute the gradient of the tuning curve\n",
    "    via numerical method\n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "        tuning_curve_fun (_type_): tuning curve function\n",
    "        eta (_type_, optional): _description_. Defaults to 1e-4.\n",
    "\n",
    "    Returns:\n",
    "        _type_: gradient of tuning curve\n",
    "    \"\"\"\n",
    "    # tuning curve\n",
    "    tc = tuning_curve_fun(X, model=model)\n",
    "\n",
    "    # compute numerical derivative, nabla_x f\n",
    "    f1 = tuning_curve_fun(X + np.array([eta, 0]), model=model) # of shape (1, nx**2) # OK\n",
    "    f2 = tuning_curve_fun(X + np.array([0, eta]), model=model) # of shape (1, nx**2) # OK\n",
    "    return np.vstack([f1-tc, f2-tc]) / eta # of shape (2, nx**2)\n",
    "\n",
    "\n",
    "#  Plots -------------------------------------------------\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, c = 'tab:blue', alpha = 1, htype = 'step'):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y,s=1, color=c)\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.2\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    print(xymax)\n",
    "    lim = (np.rint((xymax+1)/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    axx = ax_histx.hist(x, bins=bins, color=c)\n",
    "    ax_histx.set_ylim(0,(axx[0].max()//100+1)*100)\n",
    "    axy = ax_histy.hist(y, bins=bins, orientation='horizontal', color=c)\n",
    "    ax_histy.set_xlim(0,(axy[0].max()//100+1)*100)\n",
    "\n",
    "\n",
    "def plot_gaussian_ellipse(mean, cov, ax, n_std=1.0, **kwargs):\n",
    "    \n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals = vals[order]\n",
    "    vecs = vecs[:, order]\n",
    "\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width, height = 2 * n_std * np.sqrt(vals)\n",
    "    \n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "# Information metrics -------------------------------------------------------\n",
    "\n",
    "def compute_ssi(X, firingrate, dx, amp=20, Sigmax=np.array([[1,0.25], [0.25,1]])):\n",
    "    \"\"\"Compute SSI for stimulus distribution with zero mean \n",
    "    and given covariance matrix\n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "        firingrate (_type_): _description_\n",
    "        dx (_type_): _description_\n",
    "        amp (int, optional): _description_. Defaults to 20.\n",
    "        Sigmax (_type_, optional): _description_. Defaults to np.array([[1,0.25], [0.25,1]]).\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    nx = int(sqrt(X.shape[0]))\n",
    "    \n",
    "    # Calculate log probability: log p(x) for multivariate Gaussian\n",
    "    # log p(x) = -0.5 * (x^T * Sigma^(-1) * x) - 0.5 * log(det(2*pi*Sigma))\n",
    "    # - inverse of covariance matrix\n",
    "    Sigmax_inv = np.linalg.inv(Sigmax)                            \n",
    "\n",
    "    # - Mahalanobis distance term: (x^T * Sigma^(-1) * x)\n",
    "    # For multiple points in X (shape: 2 x N), compute for each column\n",
    "    mahal_dist = np.sum(X.T * (Sigmax_inv @ X.T), axis=0)\n",
    "    logpx = -0.5 * mahal_dist - 0.5 * np.linalg.slogdet(2 * np.pi * Sigmax)[1]\n",
    "\n",
    "    # setup space of possible spike counts\n",
    "    r = np.arange(0, amp + ceil(sqrt(amp*5))+1)[None] # of shape (n_neuron=1,31)\n",
    "\n",
    "    # log p(r|x) of shape (nx**2, r.shape[1])\n",
    "    logpr_x =  log( vec(firingrate) )*r - vec(firingrate) - sum(gammaln(r+1))   # OK!\n",
    "\n",
    "    # log p(r,x) of shape (nx**2, r.shape[1])\n",
    "    logprx = logpr_x + vec(logpx)\n",
    "\n",
    "    # log p(r) of shape (n_neurons=1, r.shape[1])\n",
    "    logpr = logsumexp(logprx + log(dx**2), 0)[None]\n",
    "\n",
    "    # log p(x|r) of shape (nx**2, r.shape[1])\n",
    "    logpx_r = logpr_x + vec(logpx) - logpr\n",
    "\n",
    "    # H(X|r) of shape (n_neurons=1,31)\n",
    "    HX_r = (-sum( exp(logpx_r) * logpx_r) * dx**2)[None]\n",
    "\n",
    "    # SSI = H(X) - <H(X|r)>_p(r|x) of shape (nx, nx)\n",
    "    # for a stimulus distribution with zero mean Gaussian with covariance matrix \n",
    "    ssi = 0.5 * slogdet(2*pi*exp(1)*Sigmax)[1] - HX_r @ exp(logpr_x).T                      # OK!\n",
    "    ssi = ssi.reshape(nx,nx).T\n",
    "\n",
    "    # I(R;X), a scalar\n",
    "    Inf_ssi = vec(ssi).T @ exp(vec(logpx)) * dx**2\n",
    "    return ssi, Inf_ssi, logpx, logpr_x, logpr\n",
    "\n",
    "\n",
    "# unit-testing --------------------------------------------------\n",
    "\n",
    "def test_f(x):\n",
    "\n",
    "    # x points to plot, from -xmax to xmax\n",
    "    nx = 100\n",
    "    xmax = 10\n",
    "    x = np.linspace(-xmax, xmax, nx)[None] # of shape (1,100)\n",
    "    dx = x[0,1] - x[0,0]\n",
    "\n",
    "    # transform into 2D stimulus (capitalized to show 2D)\n",
    "    xtemp1, xtemp2 = np.meshgrid(x, x)\n",
    "    X = np.column_stack((vec(xtemp1), vec(xtemp2))) # of shape (nx**2, 2)    \n",
    "\n",
    "    # Parameters\n",
    "    amp = 20                         # maximum firing rate\n",
    "    Sigma_f = np.array([[1, 0.2], \n",
    "                        [0.2, 0.5]])\n",
    "    Sigma_f2 = np.array([[1, -0.4], \n",
    "                        [-0.4, 0.5]])\n",
    "    mu = np.array([0.3, -1.2])\n",
    "\n",
    "    # test\n",
    "    assert np.allclose(np.exp(-0.5 * sum(X.T * solve(Sigma_f, X.T))).mean(), 0.0104, atol=1e-4), \"wrong\"\n",
    "    assert np.allclose(tuning_curve_mog(X, mu, Sigma_f, Sigma_f2, amp).mean(), 2.1942)\n",
    "\n",
    "\n",
    "def test_ssi(ssi, inf_ssi, logpx, logpr_x, logpr):\n",
    "\n",
    "    assert np.allclose(logpx[:10], np.array([-81.80560781, -80.21121268, -78.66035052, -77.15302134,\n",
    "    -75.68922512, -74.26896189, -72.89223162, -71.55903433,\n",
    "    -70.26937001, -69.02323866])), \"wrong logpx\"\n",
    "\n",
    "    assert np.allclose(logpr_x[:3,:3], np.array([[-2.        , -1.30685282, -1.30685282],\n",
    "    [-2.        , -1.30685282, -1.30685282],\n",
    "    [-2.        , -1.30685282, -1.30685282]])), \"wrong logpr_x\"\n",
    "\n",
    "    assert np.allclose(logpr, np.array([ -4.004177  ,  -3.0262503 ,  -2.6462648 ,  -2.540976  ,\n",
    "            -2.55821745,  -2.61336901,  -2.66757795,  -2.71162076,\n",
    "            -2.74981398,  -2.79006207,  -2.84012527,  -2.90677374,\n",
    "            -2.99569149,  -3.11148839,  -3.25774536,  -3.43710368,\n",
    "            -3.65139045,  -3.90175929,  -4.18882754,  -4.51279853,\n",
    "            -4.87356435,  -5.27078859,  -5.7039714 ,  -6.17249935,\n",
    "            -6.67568333,  -7.21278685,  -7.78304701,  -8.38568994,\n",
    "            -9.01994198,  -9.68503768, -10.38022546])), \"Wrong logpr\"\n",
    "\n",
    "    # SSI\n",
    "    assert np.allclose(ssi.mean(), -0.1210, atol=1e-4), \"wrong SSI\"\n",
    "    assert np.allclose(inf_ssi, 0.5991, atol=1e-4), \"wrong Inf_ssi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 833 ms, sys: 88 ms, total: 921 ms\n",
      "Wall time: 919 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load one cell data\n",
    "mat = sio.loadmat(CELL_DATA_PATH)\n",
    "np.random.seed(10)\n",
    "\n",
    "# get tuning curve data\n",
    "pcs = mat['X_lowd'] # principal components\n",
    "fit = mat['f']      # average firing rate (tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior of PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 0.2139, Val Loss: 0.2523\n",
      "Epoch 20/200, Train Loss: 0.0625, Val Loss: 0.1091\n",
      "Epoch 40/200, Train Loss: 0.0485, Val Loss: 0.0953\n",
      "Epoch 60/200, Train Loss: 0.0411, Val Loss: 0.0864\n",
      "Epoch 80/200, Train Loss: 0.0366, Val Loss: 0.0807\n",
      "Epoch 100/200, Train Loss: 0.0332, Val Loss: 0.0766\n",
      "Epoch 120/200, Train Loss: 0.0307, Val Loss: 0.0737\n",
      "Epoch 140/200, Train Loss: 0.0288, Val Loss: 0.0712\n",
      "Epoch 160/200, Train Loss: 0.0274, Val Loss: 0.0692\n",
      "Epoch 180/200, Train Loss: 0.0263, Val Loss: 0.0672\n",
      "Epoch 200/200, Train Loss: 0.0255, Val Loss: 0.0659\n",
      "mse shape:  (301, 301)\n",
      "grid shape:  (301, 301)\n",
      "Loaded precomputed data: KeysView(NpzFile 'data/computed_contrast_cells/BDEvSSI_no2.npz' with keys: mse, posterior, ssi, grid_x, grid_y)\n",
      "CPU times: user 5.12 s, sys: 77.2 ms, total: 5.2 s\n",
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# setup mean and covariance of the prior -----------------------------------\n",
    "\n",
    "mu0 = pcs[0].mean()\n",
    "mu1 = pcs[1].mean()\n",
    "sigma = np.cov(pcs[0], pcs[1])\n",
    "baseline = 1e-2\n",
    "\n",
    "# get tuning curve  -------------------------------\n",
    "\n",
    "# fit a neural net to RGCs response data to (PC1,PC2) to get a response function to (PC1, PC2)\n",
    "# fit data\n",
    "# _, model, _, grid_x, grid_y, grid_input_scaled, grid_input = nnet_fit(pcs[:2], fit, linspace=np.linspace(-10,10,100))\n",
    "_, model, _, grid_x, grid_y, grid_input_scaled, grid_input = nnet_fit(pcs[:2], fit)\n",
    "\n",
    "# get average responses (tuning curve)\n",
    "preds = tuning_curve_nnet(grid_input, model)\n",
    "\n",
    "# preds, model, _, grid_x, grid_y, grid_input_scaled, grid_input = tuning_curve_nnet(pcs[:2], fit)\n",
    "tc = preds.reshape(grid_x.shape) + baseline\n",
    "\n",
    "# compute/load neural information metrics  ------------------------\n",
    "\n",
    "# compute prior\n",
    "mus = pcs[:2].mean(axis=1)\n",
    "prior = multivariate_normal(mus, sigma)\n",
    "grid_prior = prior.pdf(grid_input).reshape(grid_x.shape)/prior.pdf(grid_input).reshape(grid_x.shape).sum()\n",
    "prior_entropy = -np.sum(grid_prior*np.around(np.log2(grid_prior), 8))\n",
    "\n",
    "# compute likelihood\n",
    "likelihood = POISSON_2DCELL(tc)\n",
    "\n",
    "# compute posterior\n",
    "evidence = np.sum(likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1)), axis=(-1,-2))\n",
    "posterior = likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1))/\\\n",
    "                        np.tile(evidence[:,None,None], (1,*likelihood.shape[1:])) \n",
    "posterior[posterior==0] = 1e-50\n",
    "posterior = posterior/np.tile(posterior.sum(axis=(1,2))[:,None,None], (1, *grid_x.shape))\n",
    "\n",
    "# computing is slow for 300 x 300 grid (> 1 hour)\n",
    "if not LOAD_METRICS:\n",
    "\n",
    "    # compute SSI\n",
    "    post_entropy = -np.sum(posterior*np.around(np.log2(posterior),8), axis=(1,2))\n",
    "    ssi = prior_entropy - np.sum(likelihood * np.tile(post_entropy[:,None,None], (1, *grid_x.shape)) ,axis=0)\n",
    "\n",
    "    # compute Bayes error\n",
    "    mse = np.empty_like(posterior[0])\n",
    "    for i in range(likelihood.shape[1]):\n",
    "        for j in range(likelihood.shape[2]):\n",
    "            post_ij = np.sum(posterior * np.tile(likelihood[:,i,j][:,None,None], (1, *likelihood.shape[1:])), axis=0)\n",
    "            delta = (grid_x - grid_x[i,j])**2+(grid_y-grid_y[i,j])**2\n",
    "            mse[i,j] = np.sum(post_ij*delta)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"Computed SSI and Bayes error.\")\n",
    "\n",
    "else: \n",
    "    # load precomputed metrics\n",
    "    out = np.load(METRICS_DATA_PATH)\n",
    "    ssi = out['ssi']\n",
    "    mse = out['mse']\n",
    "    rmse = np.sqrt(out['mse'])\n",
    "    print(\"mse shape: \", mse.shape)\n",
    "    print(\"grid shape: \", out['grid_x'].shape)\n",
    "    print(\"Loaded precomputed data:\", out.keys())\n",
    "\n",
    "# compute SSI bound\n",
    "ssi_bound = 2**(prior_entropy - ssi-2*np.log2(50))/(np.sqrt(2*np.pi*math.e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D-stimulus X(PC1,PC2) -------------------------\n",
    "\n",
    "# x points to plot, from -xmax to xmax\n",
    "nx = grid_x.shape[0]\n",
    "xmax = grid_input.max() # 10 - OK! = 10 for grid linspace on -10 to 10 with 100 steps\n",
    "x = np.linspace(-xmax, xmax, nx)[None] # of shape (1,100)\n",
    "dx = x[0,1] - x[0,0]              # note: dx is 10 X smaller than in replication - OK! = 0.202\n",
    "\n",
    "# get the space of possible values of the first two PCs (2D-stimulus)\n",
    "X = grid_input[:,::-1] # invert column order # shape (nx**2,2)\n",
    "\n",
    "# create neuron 2D tuning curve f(PC1,PC2) ----------------------------\n",
    "\n",
    "# firingrate = tuning_curve_mog(grid_input).reshape(nx,nx).T # shape (nx,nx) \n",
    "rate_preds = tuning_curve_nnet(grid_input, model)\n",
    "firingrate = rate_preds.reshape(nx,nx)\n",
    "\n",
    "# compute ssi ------------------------------------\n",
    "\n",
    "# for stimulus distribution (p(x)) with zero mean and given covariance\n",
    "ssi, Inf_ssi, logpx, logpr_x, logpr = compute_ssi(X, firingrate, dx, amp=20, Sigmax=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit-test\n",
    "# test_ssi(SSI, Inf_ssi, logpx, logpr_x, logpr) # works only for default parameters of the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $I_{local}$\n",
    "\n",
    "Execution time: 17 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# setup parameters\n",
    "# - integrate J_gamma(y) dgamma, b/w gamma=0 and gamma=gamma0\n",
    "\n",
    "# limits for integration - this may need tweaking depending on snr etc.\n",
    "ngamma = 20      # default=100;\n",
    "gamma0 = 0.01\n",
    "gamma_max = 200\n",
    "eta = 1e-4\n",
    "ny = 101         # discretisation of y = x + sqrt(gamma)*noise. Adapt depending on gamma\n",
    "amp = 20         # max possible spike count\n",
    "\n",
    "# compute Ilocal ------------------------------------------------------------\n",
    "\n",
    "# Ilocal0 =  int J_gamma(y) dgamma, from gamma_0 to gamma\n",
    "Ilocal0 = zeros((nx**2, 1))\n",
    "\n",
    "# compute the gradient of the tuning curve\n",
    "# df = get_tuning_curve_grad(grid_input, tuning_curve_mog, eta=1e-4)        # for toy MOG model\n",
    "df = get_tuning_curve_grad(grid_input, tuning_curve_nnet, model, eta=1e-4)  # for fitted RGC responses\n",
    "\n",
    "# loop over x\n",
    "for i in range(nx**2):\n",
    "    J = df[:,i][:,None] * df[:,i][:,None].T / rate_preds[i] # fisher\n",
    "    Ilocal0[i] = 0.5 * slogdet(eye(2) + J * gamma0 * sigma / (gamma0*eye(2)+sigma))[1]   #Ilocal0\n",
    "Ilocal0 = Ilocal0.reshape(nx,nx).T\n",
    "\n",
    "# set the number of gamma to loop through\n",
    "gamma = exp(linspace(log(gamma0), log(gamma_max), ngamma))\n",
    "dgamma = np.hstack([gamma[0], np.diff(gamma)]) # gamma step size\n",
    "\n",
    "# Jx = <J_gamma(y)>_p(y|x)\n",
    "meanJry = zeros((nx**2, ngamma))\n",
    "\n",
    "# initialize ilocal\n",
    "Inf = zeros((ngamma, ngamma))\n",
    "\n",
    "# loop over noise scales\n",
    "for igamma in range(len(gamma)):\n",
    "    \n",
    "    # discretisation of y = x+ sqrt(gamma)*noise. Adapt depending on gamma\n",
    "    ymax = xmax + 5 * sqrt(gamma[igamma]) # ok\n",
    "    y = linspace(-ymax, ymax, ny) # ok\n",
    "    dy = y[1] - y[0] # ok\n",
    "    \n",
    "    [ycord1, ycord2] = meshgrid(y, y)  # ok\n",
    "    Y = np.hstack([vec(ycord1), vec(ycord2)]) # ok\n",
    "    \n",
    "    ## firing rate in this new space\n",
    "    # firingrate = f(Y).reshape(ny, ny).T # Ok!\n",
    "    rate_preds = tuning_curve_nnet(Y, model).reshape(ny,ny)     # Ok!\n",
    "\n",
    "    # log p(x) in this new coordinate space\n",
    "    logpx = -0.5 * sum(Y.T * solve(sigma, Y.T)).reshape(ny,ny).T - 0.5 * slogdet(2*pi*sigma)[1] # OK!\n",
    "\n",
    "    # p(y|x=0), gaussian filter for convolution\n",
    "    phi = exp(-0.5*(ycord1**2 + ycord2**2)/gamma[igamma]) / (2*pi*gamma[igamma]) # Ok!\n",
    "\n",
    "    # p(y|x=eta) same as above, but increment x, for numerical derivative\n",
    "    phi1 = exp(-0.5*((ycord1+eta)**2+ycord2**2)/gamma[igamma]) / (2*pi*gamma[igamma]) # Ok!\n",
    "    phi2 = exp(-0.5*(ycord1**2+(ycord2+eta)**2)/gamma[igamma]) / (2*pi*gamma[igamma]) # Ok!\n",
    "\n",
    "    # convolve p(x) with phi to get p(y)\n",
    "    logpy = log( convolve2d(exp(logpx), phi, 'same')*(dy**2)) # Ok!\n",
    "\n",
    "    # same thing, but with incremented x, for numerical derivative\n",
    "    logpy1 = log( convolve2d(exp(logpx), phi1, 'same')*(dy**2)) # Ok!\n",
    "    logpy2 = log( convolve2d(exp(logpx), phi2, 'same')*(dy**2)) # Ok!\n",
    "\n",
    "    # J_R(y)\n",
    "    Jry = zeros((ny, ny))\n",
    "    \n",
    "    # loop over spike counts\n",
    "    for r in range(int(amp+ceil(sqrt(amp*5))+1)): \n",
    "\n",
    "        # log p(r|x)\n",
    "        logpr_x = log(rate_preds)*r - rate_preds - gammaln(r+1)  # OK\n",
    "\n",
    "        # p(r,x)\n",
    "        prx = exp(logpr_x + logpx) # Ok!\n",
    "\n",
    "        # convolve p(r,x) with phi(x) to get log p(r,y), then subtract\n",
    "        # logp(y) to get log p(r|y)\n",
    "        logpr_y = log( convolve2d(prx, phi, 'same')*(dy**2)) - logpy # Ok!\n",
    "\n",
    "        # same as above but with incremented x, for numerical derivative\n",
    "        logpr_y1 = log( convolve2d(prx, phi1, 'same')*(dy**2)) - logpy1 # Ok!\n",
    "        logpr_y2 = log( convolve2d(prx, phi2, 'same')*(dy**2)) - logpy2 # Ok!\n",
    "\n",
    "        # Jry = <(d logpr_y/dy)^2>_{p(r|y)} - sum over p(r|y)\n",
    "        Jry = Jry + exp(logpr_y) * ((logpr_y2 - logpr_y)**2 + (logpr_y1 - logpr_y)**2)/(eta**2) # Ok!\n",
    "\n",
    "    # p(y|x) - to go back to space of 'grid_input'  of shape (nx**2, 10201)\n",
    "    py_x = exp(-0.5*(sum2(X**2) - 2*X @ Y.T + sum2(Y**2).T)/gamma[igamma])/(2*pi*gamma[igamma]) # Ok!\n",
    "\n",
    "    # meanJry_gamma(x) = <J_gamma(y)>_p(y|x) of shape (nx**2, len(gamma))\n",
    "    meanJry[:, igamma] = (py_x @ vec(Jry) * (dy**2)).flatten() # Ok!\n",
    "\n",
    "    # take numerical integral over gamma, to get Ilocal\n",
    "    if igamma > 0:\n",
    "        Ilocal = 0.5 * trapezoid(y=meanJry[:,:igamma+1], x=gamma[:igamma+1], axis=1).reshape(nx,nx).T + Ilocal0 # OK!\n",
    "    else:\n",
    "        Ilocal = Ilocal0\n",
    "\n",
    "    # estimates\n",
    "    logpx = -0.5*sum(X.T * solve(sigma, X.T)) - 0.5*slogdet(2*pi*sigma)[1] # of shape (nx**2,1) # Ok!\n",
    "    Inf[igamma] = vec(Ilocal).T @ exp(vec(logpx)) * (dx**2)  # of shape (len(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10000 into shape (301,301)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10000 into shape (301,301)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# setup parameters\n",
    "xylim = (-3,3)\n",
    "xyticks = (-3,0,3)\n",
    "\n",
    "# Create mask for 3 standard deviations\n",
    "mask_3std = create_gaussian_mask(out['grid_x'], out['grid_y'], np.array([mu0, mu1]), sigma, n_std=3)\n",
    "\n",
    "# Apply mask to data\n",
    "preds_masked = np.where(mask_3std, preds.reshape(out['grid_x'].shape), np.nan)\n",
    "ssi_masked = np.where(mask_3std, ssi, np.nan)\n",
    "Ilocal_masked = np.where(mask_3std, Ilocal, np.nan)\n",
    "\n",
    "# setup plot\n",
    "fig = plt.figure(figsize=(1.7,8))\n",
    "\n",
    "# create main GridSpec: 1 col, 3 rows\n",
    "gs_main = gridspec.GridSpec(4,1, figure=fig, wspace=0, height_ratios=[1.5,1,1,1])\n",
    "\n",
    "# First subplot: Stimulus pcs & histogram --------------------------------------------------------------------------------\n",
    "gs_top = gridspec.GridSpecFromSubplotSpec(2, 2,\n",
    "                                          subplot_spec=gs_main[0],\n",
    "                                          width_ratios=[4, 1],\n",
    "                                          height_ratios=[1, 4],\n",
    "                                          hspace=0.5, wspace=0.2)\n",
    "\n",
    "# create axes for the mosaic\n",
    "axs = {}\n",
    "axs['histx'] = fig.add_subplot(gs_top[0, 0])\n",
    "axs['scatter'] = fig.add_subplot(gs_top[1, 0])\n",
    "axs['histy'] = fig.add_subplot(gs_top[1, 1])\n",
    "\n",
    "# plot scatter points and histograms\n",
    "scatter_hist(pcs[0], pcs[1], axs['scatter'], axs['histx'], axs['histy'], c=(.7,0.7,0.7))\n",
    "axs['scatter'].set_aspect('equal')\n",
    "axs['scatter'].set_xlabel('Natural image PC1')\n",
    "axs['scatter'].set_ylabel('Natural image PC2')\n",
    "axs['scatter'].set_xlim(xylim)\n",
    "axs['scatter'].set_ylim(xylim)\n",
    "axs['scatter'].set_xticks(xyticks,xyticks)\n",
    "axs['scatter'].set_yticks(xyticks,xyticks)\n",
    "\n",
    "# plot prior (contours)\n",
    "for n_std in np.arange(0, 5, 1):\n",
    "    plot_gaussian_ellipse(np.array([mu0, mu1]), sigma,\n",
    "                         axs['scatter'], n_std=n_std,\n",
    "                         edgecolor='red', facecolor='None')\n",
    "\n",
    "# aesthetics\n",
    "axs['scatter'].spines[['top','right']].set_visible(False)\n",
    "axs['histx'].spines[['top','right']].set_visible(False)\n",
    "axs['histx'].set_ylabel('Count')\n",
    "axs['histy'].spines[['top','right']].set_visible(False)\n",
    "axs['histy'].set_xlabel('Count')\n",
    "\n",
    "# Second subplot: Tuning curve and prior --------------------------------------------------------------------------------\n",
    "\n",
    "ax_bottom = fig.add_subplot(gs_main[1])\n",
    "\n",
    "# plot tuning curve as heatmap (MASKED - white outside 3 std)\n",
    "im = ax_bottom.contourf(grid_x, grid_y, preds_masked, levels=50, cmap='viridis', extend='neither')\n",
    "ax_bottom.set_facecolor('white')  # Set background to white for masked regions\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax_bottom)\n",
    "cax = divider.append_axes(\"right\", size=\"10%\", pad=0.3)\n",
    "cbar = plt.colorbar(im, cax=cax, label=\"mean spike count\")\n",
    "\n",
    "# plot prior as contours\n",
    "for n_std in np.arange(0, 6, 1):\n",
    "    plot_gaussian_ellipse(np.array([mu0, mu1]), sigma,\n",
    "                         ax_bottom, n_std=n_std,\n",
    "                         edgecolor='red', facecolor='None')\n",
    "\n",
    "# formatting\n",
    "ax_bottom.set_aspect('equal')\n",
    "ax_bottom.spines[['right']].set_visible(False)\n",
    "ax_bottom.set_xlabel(\"Natural image PC1\")\n",
    "ax_bottom.set_ylabel(\"Natural image PC2\")\n",
    "ax_bottom.set_xlim(xylim)\n",
    "ax_bottom.set_ylim(xylim)\n",
    "ax_bottom.set_xticks(xyticks,xyticks)\n",
    "ax_bottom.set_yticks(xyticks,xyticks)\n",
    "ax_bottom.spines[['top','right']].set_visible(False)\n",
    "\n",
    "# Third subplot: plot SSI (MASKED - white outside 3 std) ----------------------------------------------------\n",
    "ax_bottom3 = fig.add_subplot(gs_main[2])\n",
    "im = ax_bottom3.contourf(out['grid_x'], out['grid_y'], ssi_masked/log(2), levels=50, cmap='viridis', extend='neither')\n",
    "\n",
    "# formatting\n",
    "ax_bottom3.set_facecolor('white')  # Set background to white for masked regions\n",
    "ax_bottom3.set_xlim(xylim)\n",
    "ax_bottom3.set_ylim(xylim)\n",
    "ax_bottom3.set_xticks(xyticks,xyticks)\n",
    "ax_bottom3.set_yticks(xyticks,xyticks)\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax_bottom3)\n",
    "cax = divider.append_axes(\"right\", size=\"10%\", pad=0.3)\n",
    "cbar = plt.colorbar(im, cax=cax, label='SSI (bits)')\n",
    "\n",
    "# plot prior as contours\n",
    "for n_std in np.arange(0, 6, 1):\n",
    "    plot_gaussian_ellipse(np.array([mu0, mu1]), sigma,\n",
    "                         ax_bottom3, n_std=n_std,\n",
    "                         edgecolor='red', facecolor='None')\n",
    "# formatting\n",
    "ax_bottom3.set_xlabel(\"Natural image PC1\")\n",
    "ax_bottom3.set_ylabel(\"Natural image PC2\")\n",
    "ax_bottom3.set_aspect('equal')\n",
    "ax_bottom3.spines[['top','right']].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fourth subplot: plot Ilocal (MASKED - white outside 3 std) ----------------------------------------------------\n",
    "ax_bottom3 = fig.add_subplot(gs_main[3])\n",
    "im = ax_bottom3.contourf(out['grid_x'], out['grid_y'], Ilocal_masked / log(2), levels=50, cmap='viridis', extend='neither')\n",
    "\n",
    "# formatting\n",
    "ax_bottom3.set_facecolor('white')  # Set background to white for masked regions\n",
    "ax_bottom3.set_xlim(xylim)\n",
    "ax_bottom3.set_ylim(xylim)\n",
    "ax_bottom3.set_xticks(xyticks,xyticks)\n",
    "ax_bottom3.set_yticks(xyticks,xyticks)\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax_bottom3)\n",
    "cax = divider.append_axes(\"right\", size=\"10%\", pad=0.3)\n",
    "cbar = plt.colorbar(im, cax=cax, label='$I_{local}$ (bits)')\n",
    "\n",
    "# plot prior as contours\n",
    "for n_std in np.arange(0, 6, 1):\n",
    "    plot_gaussian_ellipse(np.array([mu0, mu1]), sigma,\n",
    "                         ax_bottom3, n_std=n_std,\n",
    "                         edgecolor='red', facecolor='None')\n",
    "# formatting\n",
    "ax_bottom3.set_xlabel(\"Natural image PC1\")\n",
    "ax_bottom3.set_ylabel(\"Natural image PC2\")\n",
    "ax_bottom3.set_aspect('equal')\n",
    "ax_bottom3.spines[['top','right']].set_visible(False)\n",
    "\n",
    "# format figure\n",
    "fig.subplots_adjust(wspace=0.9, hspace=0.5)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('figures/fig3.svg', bbox_inches = 'tight', transparent=True, dpi=400)\n",
    "plt.savefig('figures/fig3.pdf', bbox_inches = 'tight', transparent=True, dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control I_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 147 ms, sys: 10.8 ms, total: 158 ms\n",
      "Wall time: 157 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# plot convergence\n",
    "plt.figure(2, figsize=(2,2))\n",
    "\n",
    "plt.semilogx(gamma[:igamma], Inf[:igamma], 'b-', linewidth=2, label='plug in')\n",
    "\n",
    "plt.axhline(Inf_ssi, color='orange', linestyle='--', linewidth=2, \n",
    "            label=r'$\\int_0^\\gamma \\langle I_{\\gamma}(x)\\rangle_{p(x)} d\\gamma$')\n",
    "\n",
    "plt.xlabel('gamma', fontsize=12)\n",
    "plt.ylabel('information (nats)', fontsize=12)\n",
    "plt.ylim([0, 1.2 * Inf_ssi])\n",
    "plt.xlim([gamma[0], gamma[igamma-1]])\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/fig3_info_converg.svg', bbox_inches = 'tight', transparent=True, dpi=400)\n",
    "plt.savefig('figures/fig3_info_converg.pdf', bbox_inches = 'tight', transparent=True, dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes error & ssi for natural images\n",
    "\n",
    "purpose: computes each cell's bayesian decoding error and ssi for natural images.\n",
    "\n",
    "\n",
    "execution time: 4 min per cell\n",
    "\n",
    "## Setup \n",
    "\n",
    "activate fisher_info_limits2\n",
    "\n",
    "```python\n",
    "conda activate envs/fisher_info_limits2\n",
    "python -m ipykernel install --user --name fisher_info_limits2 --display-name \"fisher_info_limits2\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as  plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import wilcoxon, ks_2samp, multivariate_normal\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from numba import njit, prange\n",
    "import os\n",
    "import re\n",
    "\n",
    "# setup project path\n",
    "proj_path = \"/home/steeve/steeve/idv/code/fisher-info-limits/\"\n",
    "\n",
    "# setup reproducibility \n",
    "np.random.seed(10)\n",
    "\n",
    "# setup paths\n",
    "CELLS_PATH = os.path.join(proj_path, 'data/contrast_cells/') # path containing cells .mat files (e.g., carlo_data_cellno2.mat? (205 MB))\n",
    "DATA_PATH = os.path.join(proj_path, 'data/computed_contrast_cells/') # path containing cells .mat files (e.g., carlo_data_cellno2.mat? (205 MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, c = 'tab:blue', alpha = 1, htype = 'step'):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y,s=1, color=c)\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.2\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    print(xymax)\n",
    "    lim = (np.rint((xymax+1)/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    axx = ax_histx.hist(x, bins=bins)\n",
    "    ax_histx.set_ylim(0,(axx[0].max()//100+1)*100)\n",
    "    axy = ax_histy.hist(y, bins=bins, orientation='horizontal')\n",
    "    ax_histy.set_xlim(0,(axy[0].max()//100+1)*100)\n",
    "\n",
    "\n",
    "def plot_gaussian_ellipse(mean, cov, ax, n_std=1.0, **kwargs):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals = vals[order]\n",
    "    vecs = vecs[:, order]\n",
    "\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width, height = 2 * n_std * np.sqrt(vals)\n",
    "    \n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "def SUM_LOG_LIST(position):\n",
    "    '''Given an integer n it recursively calculates log(n!)'''\n",
    "    if position == 0:\n",
    "        return np.array([0])\n",
    "    if position == 1:\n",
    "        return np.append(SUM_LOG_LIST(0), 0)\n",
    "    new_list = SUM_LOG_LIST(position-1)\n",
    "    return np.append(new_list, new_list[-1]+np.around(np.log(float(position)), 8))\n",
    "\n",
    "\n",
    "def POISSON_2DCELL(tc_grid, max_firing=20):\n",
    "    log_list = np.tile(SUM_LOG_LIST(max_firing)[:,None,None], tc_grid.shape)\n",
    "    log_tc = np.around(np.log(tc_grid), 8)#, where=(mask==1), out = np.ones_like(tc_grid)*-100)\n",
    "    log_likelihood = (np.array([(i*log_tc-tc_grid) for i in range(max_firing+1)])-log_list)\n",
    "#     log_likelihood[0, mask==0]=0\n",
    "#     log_likelihood[1:, mask==0]=-np.inf\n",
    "    likelihood = np.exp(log_likelihood)\n",
    "    likelihood = likelihood/np.sum(likelihood, axis=0)\n",
    "    return likelihood    \n",
    "\n",
    "    \n",
    "# Define the network\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlo_data_cellno2.mat\n",
      "0 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 0.1037, Val Loss: 0.1473\n",
      "Epoch 20/200, Train Loss: 0.0473, Val Loss: 0.0904\n",
      "Epoch 40/200, Train Loss: 0.0356, Val Loss: 0.0761\n",
      "Epoch 60/200, Train Loss: 0.0305, Val Loss: 0.0712\n",
      "Epoch 80/200, Train Loss: 0.0276, Val Loss: 0.0673\n",
      "Epoch 100/200, Train Loss: 0.0259, Val Loss: 0.0644\n",
      "Epoch 120/200, Train Loss: 0.0248, Val Loss: 0.0627\n",
      "Epoch 140/200, Train Loss: 0.0240, Val Loss: 0.0612\n",
      "Epoch 160/200, Train Loss: 0.0234, Val Loss: 0.0605\n",
      "Epoch 180/200, Train Loss: 0.0229, Val Loss: 0.0599\n",
      "Epoch 200/200, Train Loss: 0.0225, Val Loss: 0.0592\n",
      "300 300\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loop over the cells contained in the data path\n",
    "for i,input_filename in enumerate(os.listdir(CELLS_PATH)):\n",
    "    \n",
    "    if '.mat' not in input_filename:\n",
    "        continue\n",
    "    \n",
    "    # print the name of the cell data file\n",
    "    print(input_filename)\n",
    "\n",
    "    # ensure it contains cell number \"no\"\n",
    "    match = re.search(r\"no\\d+\", input_filename)\n",
    "    cell_no = match.group()\n",
    "\n",
    "    # setup the data filename to save \n",
    "    output_filename = f'BDEvSSI_{cell_no}.npz'\n",
    "\n",
    "    # load the cell data\n",
    "    mat = sio.loadmat(os.path.join(CELLS_PATH, input_filename))\n",
    "    #bases = mat['U']\n",
    "    imgs = mat['X']     # sl - image\n",
    "    pcs = mat['X_lowd'] # sl - all principal components\n",
    "    fit = mat['f']      # sl - cell tuning curve (average response or receptive field?)\n",
    "    # spikes = mat['r']   # sl - spikes\n",
    "    print(i, imgs.shape)\n",
    "    #     plt.matshow(imgs)\n",
    "\n",
    "    # calculate average and variance of \n",
    "    # principal component 1 and 2\n",
    "    mu0 = pcs[0].mean()\n",
    "    mu1 = pcs[1].mean()\n",
    "    sigma = np.cov(pcs[0], pcs[1])\n",
    "\n",
    "    X = np.copy(pcs[:2]).T          # x and y\n",
    "    y = np.copy(np.array(fit[0]))   # r\n",
    "    \n",
    "    # normalize the inputs for better training \n",
    "    scaler = StandardScaler()   \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # create cross-validation splits\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32, requires_grad=True)\n",
    "    X_test  = torch.tensor(X_test_np, dtype=torch.float32, requires_grad=True)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "    y_test  = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # initialize model, loss, and optimizer\n",
    "    model = SimpleRegressor()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    n_epochs = 200\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 20 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            val_loss = criterion(model(X_test), y_test).item()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Inference: evaluate on a grid for visualization\n",
    "    model.eval()\n",
    "\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    # CALCULATE GRID - THIS GRID WILL BE USED THROUGHOUT\n",
    "\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(-3, 3, 301), np.linspace(-3 ,3 , 301))\n",
    "    grid_input = np.column_stack((grid_x.ravel(), grid_y.ravel()))\n",
    "    grid_input_scaled = scaler.transform(grid_input)\n",
    "    #--------------------------------------------------------------------------------------\n",
    "\n",
    "    mus = pcs[:2].mean(axis=1)\n",
    "    sigma = np.cov(pcs[:2])\n",
    "\n",
    "    prior = multivariate_normal(mus, sigma)\n",
    "\n",
    "    # grid = np.stack([grid_x.ravel(), grid_y.ravel()], axis=-1)\n",
    "    grid_prior = prior.pdf(grid_input).reshape(grid_x.shape)/prior.pdf(grid_input).reshape(grid_x.shape).sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(grid_input, dtype=torch.float32)).numpy()[:,0]\n",
    "\n",
    "    baseline = 1e-2\n",
    "    tc = preds.reshape(grid_x.shape)+baseline\n",
    "    likelihood = POISSON_2DCELL(tc)\n",
    "    evidence = np.sum(likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1)), axis=(-1,-2))\n",
    "    posterior = likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1))/\\\n",
    "                            np.tile(evidence[:,None,None], (1,*likelihood.shape[1:])) \n",
    "    posterior[posterior==0] = 1e-50\n",
    "    posterior = posterior/np.tile(posterior.sum(axis=(1,2))[:,None,None], (1, *grid_x.shape))\n",
    "\n",
    "    post_entropy = -np.sum(posterior*np.around(np.log2(posterior),8), axis=(1,2))\n",
    "    prior_entropy = -np.sum(grid_prior*np.around(np.log2(grid_prior), 8))\n",
    "    ssi = prior_entropy - np.sum(likelihood * np.tile(post_entropy[:,None,None], (1, *grid_x.shape)) ,axis=0)\n",
    "\n",
    "    # sl - compute mean squared error\n",
    "    mse = np.empty_like(posterior[0])\n",
    "    for i in range(likelihood.shape[1]):\n",
    "        for j in range(likelihood.shape[2]):\n",
    "            print(i,j, end = '\\r')    \n",
    "    #         post_ij = np.sum(prior*np.tile(likelihood[:,i,j][:,None,None], (1, *likelihood.shape[1:])), axis=0)\n",
    "            post_ij = np.sum(posterior*np.tile(likelihood[:,i,j][:,None,None], (1,*likelihood.shape[1:])), axis=0)\n",
    "            delta = (grid_x - grid_x[i,j])**2+(grid_y-grid_y[i,j])**2\n",
    "            mse[i,j] = np.sum(post_ij*delta)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # save computed data\n",
    "    np.savez(os.path.join(DATA_PATH, output_filename), mse=mse, posterior=posterior, ssi = ssi, grid_x=grid_x, grid_y=grid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as  plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import wilcoxon, ks_2samp, multivariate_normal\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from numba import njit, prange\n",
    "import os\n",
    "import re\n",
    "\n",
    "np.random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, c = 'tab:blue', alpha = 1, htype = 'step'):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y,s=1, color=c)\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.2\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    print(xymax)\n",
    "    lim = (np.rint((xymax+1)/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    axx = ax_histx.hist(x, bins=bins)\n",
    "    ax_histx.set_ylim(0,(axx[0].max()//100+1)*100)\n",
    "    axy = ax_histy.hist(y, bins=bins, orientation='horizontal')\n",
    "    ax_histy.set_xlim(0,(axy[0].max()//100+1)*100)\n",
    "\n",
    "def plot_gaussian_ellipse(mean, cov, ax, n_std=1.0, **kwargs):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals = vals[order]\n",
    "    vecs = vecs[:, order]\n",
    "\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width, height = 2 * n_std * np.sqrt(vals)\n",
    "    \n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "def SUM_LOG_LIST(position):\n",
    "    '''Given an integer n it recursively calculates log(n!)'''\n",
    "    if position == 0:\n",
    "        return np.array([0])\n",
    "    if position == 1:\n",
    "        return np.append(SUM_LOG_LIST(0), 0)\n",
    "    new_list = SUM_LOG_LIST(position-1)\n",
    "    return np.append(new_list, new_list[-1]+np.around(np.log(float(position)), 8))\n",
    "\n",
    "def POISSON_2DCELL(tc_grid, max_firing=20):\n",
    "    log_list = np.tile(SUM_LOG_LIST(max_firing)[:,None,None], tc_grid.shape)\n",
    "    log_tc = np.around(np.log(tc_grid), 8)#, where=(mask==1), out = np.ones_like(tc_grid)*-100)\n",
    "    log_likelihood = (np.array([(i*log_tc-tc_grid) for i in range(max_firing+1)])-log_list)\n",
    "#     log_likelihood[0, mask==0]=0\n",
    "#     log_likelihood[1:, mask==0]=-np.inf\n",
    "    likelihood = np.exp(log_likelihood)\n",
    "    likelihood = likelihood/np.sum(likelihood, axis=0)\n",
    "    return likelihood    \n",
    "\n",
    "    \n",
    "# Define the network\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlo_data_cellno10.mat\n",
      "0 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 4.3321, Val Loss: 4.4022\n",
      "Epoch 20/200, Train Loss: 2.1287, Val Loss: 2.0252\n",
      "Epoch 40/200, Train Loss: 1.7039, Val Loss: 1.6256\n",
      "Epoch 60/200, Train Loss: 1.5373, Val Loss: 1.5195\n",
      "Epoch 80/200, Train Loss: 1.4067, Val Loss: 1.4131\n",
      "Epoch 100/200, Train Loss: 1.3093, Val Loss: 1.3393\n",
      "Epoch 120/200, Train Loss: 1.2366, Val Loss: 1.2913\n",
      "Epoch 140/200, Train Loss: 1.1805, Val Loss: 1.2586\n",
      "Epoch 160/200, Train Loss: 1.1348, Val Loss: 1.2368\n",
      "Epoch 180/200, Train Loss: 1.0949, Val Loss: 1.2201\n",
      "Epoch 200/200, Train Loss: 1.0589, Val Loss: 1.2079\n",
      "carlo_data_cellno170.mat\n",
      "1 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 2.0321, Val Loss: 2.5623\n",
      "Epoch 20/200, Train Loss: 0.6661, Val Loss: 0.8163\n",
      "Epoch 40/200, Train Loss: 0.2682, Val Loss: 0.3250\n",
      "Epoch 60/200, Train Loss: 0.2039, Val Loss: 0.2578\n",
      "Epoch 80/200, Train Loss: 0.1840, Val Loss: 0.2432\n",
      "Epoch 100/200, Train Loss: 0.1776, Val Loss: 0.2361\n",
      "Epoch 120/200, Train Loss: 0.1752, Val Loss: 0.2335\n",
      "Epoch 140/200, Train Loss: 0.1737, Val Loss: 0.2318\n",
      "Epoch 160/200, Train Loss: 0.1726, Val Loss: 0.2304\n",
      "Epoch 180/200, Train Loss: 0.1715, Val Loss: 0.2292\n",
      "Epoch 200/200, Train Loss: 0.1707, Val Loss: 0.2283\n",
      "carlo_data_cellno50.mat\n",
      "2 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.2357, Val Loss: 1.0628\n",
      "Epoch 20/200, Train Loss: 0.2753, Val Loss: 0.2426\n",
      "Epoch 40/200, Train Loss: 0.1907, Val Loss: 0.1672\n",
      "Epoch 60/200, Train Loss: 0.1520, Val Loss: 0.1357\n",
      "Epoch 80/200, Train Loss: 0.1354, Val Loss: 0.1233\n",
      "Epoch 100/200, Train Loss: 0.1284, Val Loss: 0.1214\n",
      "Epoch 120/200, Train Loss: 0.1251, Val Loss: 0.1208\n",
      "Epoch 140/200, Train Loss: 0.1227, Val Loss: 0.1207\n",
      "Epoch 160/200, Train Loss: 0.1211, Val Loss: 0.1209\n",
      "Epoch 180/200, Train Loss: 0.1200, Val Loss: 0.1210\n",
      "Epoch 200/200, Train Loss: 0.1190, Val Loss: 0.1207\n",
      "carlo_data_cellno110.mat\n",
      "3 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.6038, Val Loss: 2.0371\n",
      "Epoch 20/200, Train Loss: 0.5234, Val Loss: 0.7735\n",
      "Epoch 40/200, Train Loss: 0.3679, Val Loss: 0.5196\n",
      "Epoch 60/200, Train Loss: 0.3025, Val Loss: 0.4608\n",
      "Epoch 80/200, Train Loss: 0.2542, Val Loss: 0.3893\n",
      "Epoch 100/200, Train Loss: 0.2132, Val Loss: 0.3415\n",
      "Epoch 120/200, Train Loss: 0.1849, Val Loss: 0.3066\n",
      "Epoch 140/200, Train Loss: 0.1679, Val Loss: 0.2863\n",
      "Epoch 160/200, Train Loss: 0.1554, Val Loss: 0.2709\n",
      "Epoch 180/200, Train Loss: 0.1459, Val Loss: 0.2586\n",
      "Epoch 200/200, Train Loss: 0.1385, Val Loss: 0.2490\n",
      "carlo_data_cellno54.mat\n",
      "4 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 3.9266, Val Loss: 4.4093\n",
      "Epoch 20/200, Train Loss: 1.3379, Val Loss: 1.4778\n",
      "Epoch 40/200, Train Loss: 0.4155, Val Loss: 0.3778\n",
      "Epoch 60/200, Train Loss: 0.3072, Val Loss: 0.3039\n",
      "Epoch 80/200, Train Loss: 0.2848, Val Loss: 0.2819\n",
      "Epoch 100/200, Train Loss: 0.2730, Val Loss: 0.2769\n",
      "Epoch 120/200, Train Loss: 0.2659, Val Loss: 0.2776\n",
      "Epoch 140/200, Train Loss: 0.2612, Val Loss: 0.2781\n",
      "Epoch 160/200, Train Loss: 0.2583, Val Loss: 0.2782\n",
      "Epoch 180/200, Train Loss: 0.2562, Val Loss: 0.2776\n",
      "Epoch 200/200, Train Loss: 0.2545, Val Loss: 0.2762\n",
      "carlo_data_cellno17.mat\n",
      "5 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.6298, Val Loss: 1.4895\n",
      "Epoch 20/200, Train Loss: 0.6848, Val Loss: 0.5599\n",
      "Epoch 40/200, Train Loss: 0.4836, Val Loss: 0.3753\n",
      "Epoch 60/200, Train Loss: 0.3984, Val Loss: 0.3103\n",
      "Epoch 80/200, Train Loss: 0.3549, Val Loss: 0.2838\n",
      "Epoch 100/200, Train Loss: 0.3318, Val Loss: 0.2780\n",
      "Epoch 120/200, Train Loss: 0.3126, Val Loss: 0.2736\n",
      "Epoch 140/200, Train Loss: 0.2976, Val Loss: 0.2720\n",
      "Epoch 160/200, Train Loss: 0.2864, Val Loss: 0.2715\n",
      "Epoch 180/200, Train Loss: 0.2775, Val Loss: 0.2715\n",
      "Epoch 200/200, Train Loss: 0.2702, Val Loss: 0.2723\n",
      "carlo_data_cellno158.mat\n",
      "6 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 0.2786, Val Loss: 0.2583\n",
      "Epoch 20/200, Train Loss: 0.1282, Val Loss: 0.1105\n",
      "Epoch 40/200, Train Loss: 0.1052, Val Loss: 0.0889\n",
      "Epoch 60/200, Train Loss: 0.0916, Val Loss: 0.0758\n",
      "Epoch 80/200, Train Loss: 0.0813, Val Loss: 0.0669\n",
      "Epoch 100/200, Train Loss: 0.0722, Val Loss: 0.0597\n",
      "Epoch 120/200, Train Loss: 0.0640, Val Loss: 0.0522\n",
      "Epoch 140/200, Train Loss: 0.0571, Val Loss: 0.0461\n",
      "Epoch 160/200, Train Loss: 0.0515, Val Loss: 0.0414\n",
      "Epoch 180/200, Train Loss: 0.0471, Val Loss: 0.0381\n",
      "Epoch 200/200, Train Loss: 0.0438, Val Loss: 0.0361\n",
      "carlo_data_cellno75.mat\n",
      "7 (108, 108, 3162)\n",
      "Epoch 1/200, Train Loss: 0.1651, Val Loss: 0.1276\n",
      "Epoch 20/200, Train Loss: 0.0060, Val Loss: 0.0076\n",
      "Epoch 40/200, Train Loss: 0.0011, Val Loss: 0.0012\n",
      "Epoch 60/200, Train Loss: 0.0004, Val Loss: 0.0005\n",
      "Epoch 80/200, Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch 100/200, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 120/200, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 140/200, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 160/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 180/200, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 200/200, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "carlo_data_cellno159.mat\n",
      "8 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.9770, Val Loss: 2.1434\n",
      "Epoch 20/200, Train Loss: 0.8467, Val Loss: 0.9189\n",
      "Epoch 40/200, Train Loss: 0.4636, Val Loss: 0.5110\n",
      "Epoch 60/200, Train Loss: 0.3408, Val Loss: 0.3950\n",
      "Epoch 80/200, Train Loss: 0.2857, Val Loss: 0.3395\n",
      "Epoch 100/200, Train Loss: 0.2649, Val Loss: 0.3220\n",
      "Epoch 120/200, Train Loss: 0.2544, Val Loss: 0.3139\n",
      "Epoch 140/200, Train Loss: 0.2459, Val Loss: 0.3077\n",
      "Epoch 160/200, Train Loss: 0.2387, Val Loss: 0.3018\n",
      "Epoch 180/200, Train Loss: 0.2326, Val Loss: 0.2964\n",
      "Epoch 200/200, Train Loss: 0.2278, Val Loss: 0.2911\n",
      "carlo_data_cellno82.mat\n",
      "9 (108, 108, 3161)\n",
      "Epoch 1/200, Train Loss: 4.0576, Val Loss: 4.9803\n",
      "Epoch 20/200, Train Loss: 2.0827, Val Loss: 2.7133\n",
      "Epoch 40/200, Train Loss: 1.4325, Val Loss: 1.8128\n",
      "Epoch 60/200, Train Loss: 1.1149, Val Loss: 1.4781\n",
      "Epoch 80/200, Train Loss: 0.8623, Val Loss: 1.1897\n",
      "Epoch 100/200, Train Loss: 0.6972, Val Loss: 1.0115\n",
      "Epoch 120/200, Train Loss: 0.6030, Val Loss: 0.8916\n",
      "Epoch 140/200, Train Loss: 0.5370, Val Loss: 0.8127\n",
      "Epoch 160/200, Train Loss: 0.4821, Val Loss: 0.7472\n",
      "Epoch 180/200, Train Loss: 0.4354, Val Loss: 0.6954\n",
      "Epoch 200/200, Train Loss: 0.3960, Val Loss: 0.6547\n",
      "carlo_data_cellno124.mat\n",
      "10 (108, 108, 3161)\n",
      "Epoch 1/200, Train Loss: 0.6307, Val Loss: 0.5990\n",
      "Epoch 20/200, Train Loss: 0.2294, Val Loss: 0.2231\n",
      "Epoch 40/200, Train Loss: 0.0912, Val Loss: 0.0968\n",
      "Epoch 60/200, Train Loss: 0.0510, Val Loss: 0.0555\n",
      "Epoch 80/200, Train Loss: 0.0351, Val Loss: 0.0381\n",
      "Epoch 100/200, Train Loss: 0.0317, Val Loss: 0.0326\n",
      "Epoch 120/200, Train Loss: 0.0301, Val Loss: 0.0310\n",
      "Epoch 140/200, Train Loss: 0.0293, Val Loss: 0.0301\n",
      "Epoch 160/200, Train Loss: 0.0288, Val Loss: 0.0296\n",
      "Epoch 180/200, Train Loss: 0.0284, Val Loss: 0.0293\n",
      "Epoch 200/200, Train Loss: 0.0281, Val Loss: 0.0292\n",
      "carlo_data_cellno57.mat\n",
      "11 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.5788, Val Loss: 1.7100\n",
      "Epoch 20/200, Train Loss: 0.4636, Val Loss: 0.4579\n",
      "Epoch 40/200, Train Loss: 0.1461, Val Loss: 0.1725\n",
      "Epoch 60/200, Train Loss: 0.0943, Val Loss: 0.1168\n",
      "Epoch 80/200, Train Loss: 0.0755, Val Loss: 0.0969\n",
      "Epoch 100/200, Train Loss: 0.0662, Val Loss: 0.0867\n",
      "Epoch 120/200, Train Loss: 0.0595, Val Loss: 0.0775\n",
      "Epoch 140/200, Train Loss: 0.0547, Val Loss: 0.0708\n",
      "Epoch 160/200, Train Loss: 0.0511, Val Loss: 0.0660\n",
      "Epoch 180/200, Train Loss: 0.0486, Val Loss: 0.0627\n",
      "Epoch 200/200, Train Loss: 0.0470, Val Loss: 0.0605\n",
      "carlo_data_cellno144.mat\n",
      "12 (108, 108, 3161)\n",
      "Epoch 1/200, Train Loss: 0.5015, Val Loss: 0.6558\n",
      "Epoch 20/200, Train Loss: 0.1768, Val Loss: 0.2483\n",
      "Epoch 40/200, Train Loss: 0.1054, Val Loss: 0.1425\n",
      "Epoch 60/200, Train Loss: 0.0773, Val Loss: 0.0931\n",
      "Epoch 80/200, Train Loss: 0.0664, Val Loss: 0.0742\n",
      "Epoch 100/200, Train Loss: 0.0611, Val Loss: 0.0648\n",
      "Epoch 120/200, Train Loss: 0.0588, Val Loss: 0.0607\n",
      "Epoch 140/200, Train Loss: 0.0576, Val Loss: 0.0593\n",
      "Epoch 160/200, Train Loss: 0.0570, Val Loss: 0.0587\n",
      "Epoch 180/200, Train Loss: 0.0565, Val Loss: 0.0584\n",
      "Epoch 200/200, Train Loss: 0.0561, Val Loss: 0.0583\n",
      "carlo_data_cellno45.mat\n",
      "13 (108, 108, 3162)\n",
      "Epoch 1/200, Train Loss: 0.3468, Val Loss: 0.3700\n",
      "Epoch 20/200, Train Loss: 0.0680, Val Loss: 0.0792\n",
      "Epoch 40/200, Train Loss: 0.0416, Val Loss: 0.0515\n",
      "Epoch 60/200, Train Loss: 0.0389, Val Loss: 0.0465\n",
      "Epoch 80/200, Train Loss: 0.0376, Val Loss: 0.0462\n",
      "Epoch 100/200, Train Loss: 0.0367, Val Loss: 0.0455\n",
      "Epoch 120/200, Train Loss: 0.0361, Val Loss: 0.0451\n",
      "Epoch 140/200, Train Loss: 0.0356, Val Loss: 0.0448\n",
      "Epoch 160/200, Train Loss: 0.0352, Val Loss: 0.0445\n",
      "Epoch 180/200, Train Loss: 0.0349, Val Loss: 0.0444\n",
      "Epoch 200/200, Train Loss: 0.0347, Val Loss: 0.0443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlo_data_cellno86.mat\n",
      "14 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 7.3573, Val Loss: 5.9294\n",
      "Epoch 20/200, Train Loss: 5.1816, Val Loss: 3.9620\n",
      "Epoch 40/200, Train Loss: 3.4606, Val Loss: 2.5411\n",
      "Epoch 60/200, Train Loss: 2.6407, Val Loss: 1.9536\n",
      "Epoch 80/200, Train Loss: 2.1385, Val Loss: 1.5341\n",
      "Epoch 100/200, Train Loss: 1.7385, Val Loss: 1.2236\n",
      "Epoch 120/200, Train Loss: 1.4171, Val Loss: 1.0000\n",
      "Epoch 140/200, Train Loss: 1.1681, Val Loss: 0.8423\n",
      "Epoch 160/200, Train Loss: 0.9716, Val Loss: 0.7246\n",
      "Epoch 180/200, Train Loss: 0.8187, Val Loss: 0.6374\n",
      "Epoch 200/200, Train Loss: 0.6961, Val Loss: 0.5670\n",
      "carlo_data_cellno133.mat\n",
      "15 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 2.6757, Val Loss: 3.4193\n",
      "Epoch 20/200, Train Loss: 1.4310, Val Loss: 1.8097\n",
      "Epoch 40/200, Train Loss: 0.7888, Val Loss: 0.9128\n",
      "Epoch 60/200, Train Loss: 0.4587, Val Loss: 0.4906\n",
      "Epoch 80/200, Train Loss: 0.2748, Val Loss: 0.2997\n",
      "Epoch 100/200, Train Loss: 0.2099, Val Loss: 0.2407\n",
      "Epoch 120/200, Train Loss: 0.1829, Val Loss: 0.2233\n",
      "Epoch 140/200, Train Loss: 0.1697, Val Loss: 0.2158\n",
      "Epoch 160/200, Train Loss: 0.1632, Val Loss: 0.2125\n",
      "Epoch 180/200, Train Loss: 0.1594, Val Loss: 0.2108\n",
      "Epoch 200/200, Train Loss: 0.1571, Val Loss: 0.2098\n",
      "carlo_data_cellno99.mat\n",
      "16 (108, 108, 3160)\n",
      "Epoch 1/200, Train Loss: 1.9062, Val Loss: 1.9619\n",
      "Epoch 20/200, Train Loss: 0.8323, Val Loss: 0.9386\n",
      "Epoch 40/200, Train Loss: 0.6928, Val Loss: 0.7774\n",
      "Epoch 60/200, Train Loss: 0.5950, Val Loss: 0.6692\n",
      "Epoch 80/200, Train Loss: 0.5248, Val Loss: 0.5894\n",
      "Epoch 100/200, Train Loss: 0.4771, Val Loss: 0.5281\n",
      "Epoch 120/200, Train Loss: 0.4453, Val Loss: 0.4855\n",
      "Epoch 140/200, Train Loss: 0.4241, Val Loss: 0.4529\n",
      "Epoch 160/200, Train Loss: 0.4085, Val Loss: 0.4243\n",
      "Epoch 180/200, Train Loss: 0.3961, Val Loss: 0.4012\n",
      "Epoch 200/200, Train Loss: 0.3863, Val Loss: 0.3819\n",
      "carlo_data_cellno60.mat\n",
      "17 (108, 108, 3161)\n",
      "Epoch 1/200, Train Loss: 0.5847, Val Loss: 0.7955\n",
      "Epoch 20/200, Train Loss: 0.2224, Val Loss: 0.3148\n",
      "Epoch 40/200, Train Loss: 0.1652, Val Loss: 0.2447\n",
      "Epoch 60/200, Train Loss: 0.1255, Val Loss: 0.1939\n",
      "Epoch 80/200, Train Loss: 0.1016, Val Loss: 0.1632\n",
      "Epoch 100/200, Train Loss: 0.0858, Val Loss: 0.1447\n",
      "Epoch 120/200, Train Loss: 0.0733, Val Loss: 0.1311\n",
      "Epoch 140/200, Train Loss: 0.0638, Val Loss: 0.1209\n",
      "Epoch 160/200, Train Loss: 0.0567, Val Loss: 0.1124\n",
      "Epoch 180/200, Train Loss: 0.0515, Val Loss: 0.1054\n",
      "Epoch 200/200, Train Loss: 0.0476, Val Loss: 0.0994\n",
      "195 520\r"
     ]
    }
   ],
   "source": [
    "for i,input_filename in enumerate(os.listdir('./Contrast_cells/')):\n",
    "    if '.mat' not in input_filename:\n",
    "        continue\n",
    "    print(input_filename)\n",
    "    match = re.search(r\"no\\d+\", input_filename)\n",
    "    cell_no = match.group()\n",
    "\n",
    "    output_filename = f'BDEvSSI_{cell_no}.npz'\n",
    "    mat = sio.loadmat(f'./Contrast_cells/{input_filename}')\n",
    "\n",
    "    bases = mat['U']\n",
    "    imgs = mat['X']\n",
    "    pcs = mat['X_lowd']\n",
    "    fit = mat['f']\n",
    "    spikes = mat['r']\n",
    "    print(i, imgs.shape)\n",
    "    #     plt.matshow(imgs)\n",
    "\n",
    "    # calculate average and variance of pc1 and pc2\n",
    "    mu0 = pcs[0].mean()\n",
    "    mu1 = pcs[1].mean()\n",
    "    sigma = np.cov(pcs[0], pcs[1])\n",
    "\n",
    "    X = np.copy(pcs[:2]).T  # x and y\n",
    "    y = np.copy(np.array(fit[0]))   # r\n",
    "    # Optional: Normalize inputs for better training\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32, requires_grad=True)\n",
    "    X_test  = torch.tensor(X_test_np, dtype=torch.float32, requires_grad=True)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "    y_test  = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = SimpleRegressor()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    n_epochs = 200\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 20 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            val_loss = criterion(model(X_test), y_test).item()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Inference: evaluate on a grid for visualization\n",
    "    model.eval()\n",
    "\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    # CALCULATE GRID - THIS GRID WILL BE USED THROUGHOUT\n",
    "\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(-3, 3, 301), np.linspace(-3 ,3 , 301))\n",
    "    grid_input = np.column_stack((grid_x.ravel(), grid_y.ravel()))\n",
    "    grid_input_scaled = scaler.transform(grid_input)\n",
    "    #--------------------------------------------------------------------------------------\n",
    "\n",
    "    mus = pcs[:2].mean(axis=1)\n",
    "    sigma = np.cov(pcs[:2])\n",
    "\n",
    "    prior = multivariate_normal(mus, sigma)\n",
    "\n",
    "    # grid = np.stack([grid_x.ravel(), grid_y.ravel()], axis=-1)\n",
    "    grid_prior = prior.pdf(grid_input).reshape(grid_x.shape)/prior.pdf(grid_input).reshape(grid_x.shape).sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(grid_input, dtype=torch.float32)).numpy()[:,0]\n",
    "\n",
    "    baseline = 1e-2\n",
    "    tc = preds.reshape(grid_x.shape)+baseline\n",
    "    likelihood = POISSON_2DCELL(tc)\n",
    "    evidence = np.sum(likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1)), axis=(-1,-2))\n",
    "    posterior = likelihood*np.tile(grid_prior[None,:,:], (likelihood.shape[0],1,1))/\\\n",
    "                            np.tile(evidence[:,None,None], (1,*likelihood.shape[1:])) \n",
    "    posterior[posterior==0] = 1e-50\n",
    "    posterior = posterior/np.tile(posterior.sum(axis=(1,2))[:,None,None], (1, *grid_x.shape))\n",
    "\n",
    "    post_entropy = -np.sum(posterior*np.around(np.log2(posterior),8), axis=(1,2))\n",
    "    prior_entropy = -np.sum(grid_prior*np.around(np.log2(grid_prior), 8))\n",
    "    ssi = prior_entropy - np.sum(likelihood * np.tile(post_entropy[:,None,None], (1, *grid_x.shape)) ,axis=0)\n",
    "\n",
    "    mse = np.empty_like(posterior[0])\n",
    "    for i in range(likelihood.shape[1]):\n",
    "        for j in range(likelihood.shape[2]):\n",
    "            print(i,j, end = '\\r')    \n",
    "    #         post_ij = np.sum(prior*np.tile(likelihood[:,i,j][:,None,None], (1, *likelihood.shape[1:])), axis=0)\n",
    "            post_ij = np.sum(posterior*np.tile(likelihood[:,i,j][:,None,None], (1,*likelihood.shape[1:])), axis=0)\n",
    "            delta = (grid_x - grid_x[i,j])**2+(grid_y-grid_y[i,j])**2\n",
    "            mse[i,j] = np.sum(post_ij*delta)\n",
    "    rmse = np.sqrt(mse)\n",
    "    np.savez(f'./Computed Contrast Cells/{output_filename}', mse=mse, posterior=posterior, ssi = ssi, grid_x=grid_x, grid_y=grid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

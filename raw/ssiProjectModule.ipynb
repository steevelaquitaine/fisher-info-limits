{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULE WITH RELEVANT PROCEDURES: NOISE MODELS, SSI CALCULATIONS FROM TCS, FIT CURVES, ...\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "from itertools import product\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import argrelextrema as arex\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "from notebook import notebookapp\n",
    "import urllib\n",
    "import json\n",
    "import os\n",
    "import ipykernel\n",
    "from shutil import copy2\n",
    "import sys\n",
    "\n",
    "from scipy.stats import poisson as poi\n",
    "\n",
    "\n",
    "def SUM_LOG_LIST(position):\n",
    "    '''Given an integer n it recursively calculates log(n!)'''\n",
    "    if position == 0:\n",
    "        return np.array([0])\n",
    "    if position == 1:\n",
    "        return np.append(SUM_LOG_LIST(0), 0)\n",
    "    new_list = SUM_LOG_LIST(position-1)\n",
    "    return np.append(new_list, new_list[-1]+np.around(np.log(float(position)), 8))\n",
    "\n",
    "def NOTEBOOK_PATH():\n",
    "    \"\"\"Returns the absolute path of the Notebook or None if it cannot be determined\n",
    "    NOTE: works only when the security is token-based or there is also no password\n",
    "    \"\"\"\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  # No token and no password, ahem...\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions?token='+srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            for sess in sessions:\n",
    "                if sess['kernel']['id'] == kernel_id:\n",
    "                    return os.path.join(srv['notebook_dir'],sess['notebook']['path'])\n",
    "        except:\n",
    "            pass  # There may be stale entries in the runtime directory \n",
    "    return None\n",
    "\n",
    "def COPY_CURRENT_NB(target_directory, filename):\n",
    "    '''Copies the notebook within which it's called in the \"target_directory\" under name \"filename\"'''\n",
    "    nb = NOTEBOOK_PATH()\n",
    "    if nb:\n",
    "        new_path =f'{target_directory}{filename}_copy.ipynb'\n",
    "        copy2(nb, new_path)\n",
    "    else:\n",
    "        print(\"Current notebook path cannot be determined.\")\n",
    "        \n",
    "\n",
    "def ADD_ARRAY_TO_NPZ(dat_dir, filename, new_array_name, new_array):\n",
    "    '''Adds an array to an existing .npz file as dictionary entry. \\n Input variables: directory of file, filename, new array name, new array'''\n",
    "\n",
    "    dictionary = dict(np.load(dat_dir+filename))\n",
    "    dictionary[new_array_name] = new_array\n",
    "    np.savez(dat_dir + filename, **dictionary)\n",
    "    return\n",
    "\n",
    "\n",
    "def GET_SSI_LOOP(all_tcs):\n",
    "    '''Input a list of TCs, \\n Output a list of associated SSIs. Output in NATS.'''\n",
    "    aux = np.array([GET_SSI_FROM_TC(el) for el in all_tcs])\n",
    "    return aux\n",
    "    \n",
    "\n",
    "def GET_SSI_FROM_TC(input_tc_):           #!!! NB: ASSUME UNIFORM PRIOR !!!\n",
    "    '''Input 1 Tuning Curve, Outputs Associated SSI, supposing Poisson noise model. Outpu in NATS.'''\n",
    "    single_tc = np.maximum(np.squeeze(input_tc_), 1e-10)\n",
    "    n_th = len(single_tc)\n",
    "    theta_entropy_ = np.full(n_th, np.log(n_th))\n",
    "    log_likelihood__, likelihood__ = POISSON(single_tc)\n",
    "    normalization_ = np.sum(likelihood__, axis = 1)\n",
    "    entropy_theta_given_m_ = np.log(normalization_) - np.sum(likelihood__*log_likelihood__, axis = 1)/normalization_\n",
    "    ssi = theta_entropy_ - np.sum(likelihood__*entropy_theta_given_m_[:,None], axis = 0)\n",
    "\n",
    "    return ssi#, log_likelihood__, likelihood__, normalization_, mask__\n",
    "\n",
    "\n",
    "def POISSON(input_tc_):     # lambda has to be poissonian, dependent on tc(sigma)\n",
    "    tc_ = np.squeeze(input_tc_)\n",
    "    n_th = len(tc_)\n",
    "    lambdas_ = np.copy(tc_)\n",
    "    log_lambdas_ = np.log(np.maximum(lambdas_, 1e-10))\n",
    "    \n",
    "    m_max_ = np.around(lambdas_+6*np.sqrt(lambdas_)+1)       #dims = dim(stim) = n_th = 36\n",
    "    \n",
    "    peak_resp = np.max(m_max_)\n",
    "    response_array_ = np.arange(peak_resp+1)            #dims = highest response integer\n",
    "    response_matrix__ = np.tile(response_array_[:,None], (1,n_th))\n",
    "#     print('r',response_matrix__)\n",
    "    mask_condition__ = np.tile(m_max_[None,:], (len(response_array_),1))\n",
    "#     print('m',mask_condition__.shape)\n",
    "    mask__ = response_matrix__ <= mask_condition__\n",
    "#     mask__ = np.ones_like(response_matrix__)\n",
    "\n",
    "    #calculate the sums of the logarithms of the reponses 0,1,2,3,4,...,m  -> [0,0,log2,log3,log4,...,logm]\n",
    "    somme_log_ = SUM_LOG_LIST(peak_resp)\n",
    "    log_likelihood__ = response_matrix__*log_lambdas_[None,:] - lambdas_[None,:] - somme_log_[:,None]\n",
    "    likelihood__ = np.exp(log_likelihood__)\n",
    "    likelihood__ = likelihood__ / np.sum(likelihood__, axis = 0)[None,:]\n",
    "    log_likelihood = np.log(np.maximum(likelihood__, 1e-10))\n",
    "    \n",
    "    log_likelihood__[mask__== False]= np.min(log_likelihood__)*10e5\n",
    "    likelihood__[mask__== False] = 0.\n",
    "    \n",
    "    return log_likelihood__, likelihood__        #return log_likelihood and likelihood\n",
    "\n",
    "def VON_MISES(th, a, th_0, s, baseline):           #indexed with 'ftvm'\n",
    "    f = lambda stimulus, amp, x_peak, width: \\\n",
    "            amp* np.exp((np.cos((stimulus-x_peak)*(np.pi/180)))/width)\n",
    "    vm_f = f(th, a, th_0, s)\n",
    "    return (vm_f / np.sum(vm_f/a) + baseline)*(1 +any(vm_f / np.sum(vm_f/a) + baseline < 0)*1e10)\n",
    "\n",
    "def FLAT_TOPPED_VON_MISES(th, a, th_0, s, baseline, g):           #indexed with 'ftvm'\n",
    "    '''Modified Von Mises function\\n Parameters Area, mean, concentration, baseline, flatness\\n g = 0 gives Von Mises function'''\n",
    "    delta_th = th[1]-th[0]\n",
    "    ftvm_f = np.exp(np.float64((np.cos(np.deg2rad(th-th_0-g*np.sin(np.deg2rad(th-th_0))))-1)/s))\n",
    "    \n",
    "    res = (a*ftvm_f / np.sum(ftvm_f/(len(th)/36)) + baseline) \n",
    "    return res+np.any(res<0.)*1e10\n",
    "\n",
    "\n",
    "def FTVM(th, a, th_0, s, baseline, g):           #indexed with 'ftvm'\n",
    "    return FLAT_TOPPED_VON_MISES(th, a, th_0, s, baseline, g)\n",
    "\n",
    "def BIG_SSI(tcs):  \n",
    "    '''Takes an array of 1,2,3 or 4 tuning curves and returns the combined SSI\\n\n",
    "    SUPPOSE NO CORRELATIONS -> p(n1,n2,...,nm|th) = p(n1|th)*p(n2|th)*...*p(nm|th)\\n\n",
    "    Output in BITS.'''\n",
    "    if tcs.ndim==1:\n",
    "#         print('s',np.squeeze(tcs))\n",
    "        return GET_SSI_FROM_TC(np.squeeze(tcs))\n",
    "\n",
    "    log_likelihoods = []\n",
    "    likelihoods = []\n",
    "    resp_dims = []\n",
    "    output_shape = []\n",
    "    for i,el in enumerate(tcs):\n",
    "        new_log_likelihood__, new_likelihood__ = POISSON(el)\n",
    "#         log_likelihoods.append(new_log_likelihood__) \n",
    "        likelihoods.append(new_likelihood__) \n",
    "        output_shape.append(len(likelihoods[i]))\n",
    "    output_shape.append(len(tcs[0]))\n",
    "#     print('os',output_shape, end = '\\r')\n",
    "    \n",
    "#    print('sparse: ',np.count_nonzero(likelihoods)/np.prod(likelihoods.shape) > 2./3)\n",
    "#    print('count1',np.count_nonzero(log_likelihoods>0))\n",
    "#    print('count2',np.count_nonzero(likelihoods<0))\n",
    "#    print('count3',np.count_nonzero(likelihoods>1))#\n",
    "\n",
    "    if len(tcs)==2:\n",
    "        multidim_likelihood = np.float32(np.einsum('im,jm->ijm', *likelihoods))          #p(n1|th)*p(n2|th)\n",
    "        multidim_norm = np.float32(np.sum(multidim_likelihood, axis=-1))\n",
    "        multidim_norm[multidim_norm==0.] = 1e-20  # CAREFUL !! IF NORM==0 THEN EVERYTHING EXPLODES -> SSI = nan\n",
    "        likelihood_entropy = np.float32(-1.*np.sum(multidim_likelihood * np.log(np.maximum(multidim_likelihood, 1e-10)), axis=-1))       # this step takes a long time !!!\n",
    "        multidim_entropy = np.float32((likelihood_entropy/multidim_norm) + np.log(multidim_norm))\n",
    "        ssi_theta = np.float32(np.log(len(tcs[0]))) - np.float32(np.sum(multidim_likelihood*multidim_entropy[:,:,None], axis = (0,1)))\n",
    "        \n",
    "\n",
    "    elif len(tcs)==3:\n",
    "        multidim_likelihood = np.array(likelihoods[0])[:, np.newaxis, np.newaxis, np.newaxis, :] \\\n",
    "                            * np.array(likelihoods[1])[np.newaxis, :, np.newaxis, np.newaxis, :] \\\n",
    "                            * np.array(likelihoods[2])[np.newaxis,np.newaxis,:, np.newaxis, :]\n",
    "        multidim_norm = np.sum(multidim_likelihood, axis=-1)\n",
    "        multidim_norm[multidim_norm == 0.] = 1e-20\n",
    "        likelihood_entropy = -1. * np.sum(multidim_likelihood * np.log(np.maximum(multidim_likelihood, 1e-10)), axis=-1)\n",
    "        multidim_entropy = (likelihood_entropy / multidim_norm) + np.log(multidim_norm)\n",
    "        del likelihood_entropy\n",
    "        del multidim_norm\n",
    "        ssi_theta = np.log(np.array(likelihoods).shape[-1]) - np.sum(multidim_likelihood * multidim_entropy[:, :, :, None], axis=(0, 1, 2))\n",
    "        del multidim_likelihood, multidim_entropy\n",
    "         \n",
    "    elif len(tcs) == 4:\n",
    "        \n",
    "        multidim_likelihood = np.array(likelihoods[0])[:, np.newaxis, np.newaxis, np.newaxis, :] \\\n",
    "                            * np.array(likelihoods[1])[np.newaxis, :, np.newaxis, np.newaxis, :] \\\n",
    "                            * np.array(likelihoods[2])[np.newaxis,np.newaxis,:, np.newaxis, :]   \\\n",
    "                            * np.array(likelihoods[3])[np.newaxis, np.newaxis, np.newaxis, :, :]\n",
    "#         print('4', end = '\\033[K\\r')\n",
    "        multidim_norm = np.sum(multidim_likelihood, axis=-1)\n",
    "        multidim_norm[multidim_norm == 0.] = 1\n",
    "#         print('3', end = '\\033[K\\r')\n",
    "\n",
    "        likelihood_entropy = -1. * np.sum(multidim_likelihood * np.log(np.maximum(multidim_likelihood, 1e-30)), axis=-1)\n",
    "#         print('2', end = '\\033[K\\r')\n",
    "        multidim_entropy = (likelihood_entropy / multidim_norm) + np.log(np.maximum(multidim_norm, 1e-30))\n",
    "#         print('1', end = '\\033[K\\r')\n",
    "        del likelihood_entropy\n",
    "        del multidim_norm\n",
    "        ssi_theta = np.log(tcs.shape[-1]) - np.sum(multidim_likelihood * multidim_entropy[:, :, :, :, None], axis=(0, 1, 2, 3))\n",
    "#         print('Done', end = '\\033[K\\r')\n",
    "        del multidim_likelihood\n",
    "        del multidim_entropy\n",
    "    del new_log_likelihood__\n",
    "    del new_likelihood__\n",
    "    del log_likelihoods\n",
    "    del output_shape\n",
    "    \n",
    "    return np.squeeze(ssi_theta)\n",
    "\n",
    "def FISHER_POISSON(stim, tcs):\n",
    "    lik_2d_spl = np.array([UnivariateSpline(stim, el, s = 0,k = 2).derivative(n=1)(stim) for el in tcs])\n",
    "    fisher = np.sum(np.array([lik_2d_spl[i]**2./tcs[i] for i in range(len(tcs))]),  axis =0)\n",
    "    return fisher\n",
    "    \n",
    "def MEAN_RES_LENGTH(stim, tcs, means, p):   #population resultant length            0 < MRL < 1\n",
    "    \"\"\"Input: stim, tcs, means, p,  Calculates the pth moment of the tcs \\n p=1 gives population resultant length \\n np.sum(tc) serves as normalization \\n input means are the means of the fitted ftvm\"\"\"\n",
    "    if tcs.ndim==1:\n",
    "        res = np.sum(tcs*np.cos(p*np.deg2rad(stim-means)))/np.sum(tcs)\n",
    "    else:\n",
    "        if np.isscalar(means):\n",
    "            res = np.array([np.sum(tc*np.cos(p*np.deg2rad(stim-means)))/np.sum(tc) for i, tc in enumerate(tcs)])   \n",
    "        else:\n",
    "            res = np.array([np.sum(tc*np.cos(p*np.deg2rad(stim-means[i])))/np.sum(tc) for i, tc in enumerate(tcs)])\n",
    "    return res\n",
    "\n",
    "def CSTD(stim, tcs, means):    #calculated the circular STD of a (flat-topped) vonMises distribution\n",
    "                               #contained between [0, infinity] for r in [1,0]=[completely centered,completely dispersed]                     \n",
    "    \"\"\" Calculates the circular STD \\n It uses the POP_RES_LENGTH internally \"\"\"\n",
    "#     b0 = np.sum(tc, axis = 1)                                #zeroth moment of tc\n",
    "    ro1 = MEAN_RES_LENGTH(stim, tcs, means, 1)                  #first moment of tc \n",
    "    cstd_rad = np.sqrt(-2*np.log(ro1))           #circular std in radians\n",
    "    return np.rad2deg(cstd_rad)                   #circular std in degrees  \n",
    "\n",
    "def SQRT_CVAR(stim, tcs, means):\n",
    "    return np.rad2deg(np.sqrt((1.-MEAN_RES_LENGTH(stim, tcs, means, 1)))*np.pi)     #between [0,180]\n",
    "\n",
    "def CDISP(stim, tcs, means):\n",
    "    ro = MEAN_RES_LENGTH(stim, tcs, means, 1)\n",
    "    ro2 = MEAN_RES_LENGTH(stim, tcs, means, 2)\n",
    "    return np.rad2deg((1.-ro2)/(2*ro**2))                #contained between [0, infinity]\n",
    "\n",
    "def ARC_DISP(stim, tcs, means):\n",
    "    '''Input: stim, tcs, means\\n gives a measure of the arcs subtracted by the tcs (circular distributions on [0,2pi])'''\n",
    "    r = MEAN_RES_LENGTH(stim, tcs, means, 1)\n",
    "    if np.isscalar(r):\n",
    "        return 2*np.rad2deg(np.arccos(r))\n",
    "    else:\n",
    "        arcs = 2*np.array([np.rad2deg(np.arccos(el)) for el in r] )\n",
    "        return 2*arcs #contained in [0,360]\n",
    "    \n",
    "    \n",
    "\n",
    "def MC_SSI(tcs_input):         #dim tcs = 4x36\n",
    "    ''' Calculates SSI by Monte Carlo method. \\n Up to 2023-11-12 gives result in nats, from 2023-11-12 (included) gives result in bits'''\n",
    "    #if input is multiple cells\n",
    "#     K = 10000\n",
    "    \n",
    "    if tcs_input.ndim==1:\n",
    "        tcs = np.atleast_2d(tcs_input)\n",
    "    else:\n",
    "        tcs = np.copy(tcs_input)\n",
    "    posterior = np.array([])\n",
    "    stim_len = tcs.shape[-1]\n",
    "    K = 100\n",
    "    err = 1e4\n",
    "    epsilon = 1        #errors in percents\n",
    "\n",
    "    previous_ssi = np.zeros(stim_len)\n",
    "    if np.max(tcs_input)<20:\n",
    "        return BIG_SSI(tcs_input)*np.log2(math.e)\n",
    "    \n",
    "    while (K<10000 and err>epsilon):\n",
    "        #get your K samples -> to be summed over at the end\n",
    "\n",
    "        sample_r = np.random.poisson(tcs, size = (K, *tcs.shape))     #dim = Kx4x36 for quadruplet\n",
    "    #     norm = np.sum(np.prod(np.maximum(poi.pmf(np.tile(sample_r[:,:,:,None], (1,1,1,stim_len)), \\\n",
    "    #                           np.tile(tcs[None,:,None, :], (K,1, stim_len,1))), 1e-10), axis = 1), axis = -1)\n",
    "        post_aux = np.maximum(1e-40, \\\n",
    "                              np.prod(np.maximum(poi.pmf(np.tile(sample_r[:,:,:,None], (1,1,1,stim_len)), \\\n",
    "                              np.tile(tcs[None,:,None, :], (K,1, stim_len,1))), 1e-100), axis = 1))\n",
    "        posterior = post_aux / np.tile(np.sum(post_aux, axis = -1)[:,:,None], (1,1,stim_len))\n",
    "\n",
    "\n",
    "        i_sp = np.log(stim_len)+np.sum(posterior*np.maximum(np.log(posterior),-50), axis = 2)\n",
    "        ssi = np.mean(i_sp, axis = 0)\n",
    "        del sample_r, post_aux, posterior\n",
    "        err = np.sqrt(np.mean((ssi-previous_ssi)**2))/np.mean(ssi)*100\n",
    "        previous_ssi = np.copy(ssi)\n",
    "        K *=2\n",
    "        \n",
    "#     print(K)\n",
    "    del previous_ssi\n",
    "    \n",
    "\n",
    "    # if the number of samples has gotten bigger than 5000 and the error is still larger than epsilon\n",
    "    # then we're probably in the high noise regime, and it's best to compute the SSI exactly - use BIG_SSI     \n",
    "    #However if the TCs have a very high maximum average firing rate, then do not compute exactly in any case.\n",
    "\n",
    "    if K>=10000 and err>epsilon and np.max(tcs_input)<30:\n",
    "        ssi = BIG_SSI(tcs_input)\n",
    "    return ssi*np.log2(math.e)\n",
    "\n",
    "\n",
    "def FWHM(pars):\n",
    "    \n",
    "    \"\"\"Return the Full Width at Half Maximum of a FTVM function.\n",
    "       The TC is obtained by subtracting the baseline and dividing by the area.\n",
    "       In other words, the measure is area and baseline independent.\n",
    "        NOTE: baseline parameter must be non-negative\n",
    "        \"\"\"\n",
    "    x = np.arange(pars[1]-180,pars[1]+180,0.5)\n",
    "    ftvm = FLAT_TOPPED_VON_MISES(x, *pars)\n",
    "\n",
    "    fcn = ftvm - np.max(ftvm)/2\n",
    "#     print(np.max(ftvm)/2)\n",
    "#     plt.figure()\n",
    "#     plt.plot(x, fcn)\n",
    "    \n",
    "    if np.all(fcn>0):\n",
    "        return np.max(ftvm)/2, 360\n",
    "    \n",
    "    idx = np.argmin(fcn**2)\n",
    "#     print(ftvm[idx])\n",
    "    return ftvm[idx], 2*np.abs(x[idx]-pars[1])%360"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
